# -*- coding: utf-8 -*-
"""LuisAlvarez_Mineria.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IpCvC7zUExpwRpCuQPNNiIW5NAEGwne1

#**EJERCICIO 1 MINERIA DE DATOS**

- Catedra: Mineria de Datos
- Periodo: 2025C
- Realizado por: Luis Alvarez
- Docente: Daniel Carrizo

**Descripci√≥n del Proyecto**

Este proyecto realiza un an√°lisis exhaustivo de los datos del departamento de Recursos Humanos de una empresa de tecnolog√≠a, con el objetivo de identificar patrones, tendencias y √°reas de oportunidad en la gesti√≥n del talento humano.

# **1: Configuraci√≥n Inicial y Carga de Datos**

Se estableci√≥ el entorno anal√≠tico importando librer√≠as especializadas para el procesamiento de datos. Pandas y NumPy se utilizaron para la manipulaci√≥n de datos tabulares y c√°lculos num√©ricos, mientras que Matplotlib y Seaborn permitieron la visualizaci√≥n de resultados. Para el an√°lisis estad√≠stico se incluyeron funciones de Scipy, y la carga del dataset se realiz√≥ mediante files.upload() de Google Colab, cargando el archivo Excel en un DataFrame de pandas con pd.read_excel('dataset_exercise.xlsx') para su posterior procesamiento.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr, mannwhitneyu, chi2_contingency
from google.colab import files

# Cargar el dataset
archivo = files.upload()
df = pd.read_excel('dataset_exercise.xlsx')

"""# **2. Diagn√≥stico Inicial del Dataset**
Se realiz√≥ un an√°lisis exploratorio inicial del dataset para comprender su estructura b√°sica. El diagn√≥stico incluy√≥ la verificaci√≥n de dimensiones, revelando la cantidad total de filas y columnas disponibles para el an√°lisis. Posteriormente, se examin√≥ la tipolog√≠a de datos mediante df.info(), identificando la distribuci√≥n de variables categ√≥ricas, num√©ricas y de fecha, lo que permiti√≥ establecer una base s√≥lida para las fases subsiguientes de limpieza y transformaci√≥n de datos.
"""

# Informaci√≥n general del dataset
print("=== INFORMACI√ìN INICIAL ===")
print(f"Dimensiones: {df.shape}")
print(f"Filas: {df.shape[0]}, Columnas: {df.shape[1]}")
print("\n=== TIPOS DE DATOS ===")
df.info()

"""#2.1 An√°lisis de Valores Faltantes
Se implement√≥ una evaluaci√≥n exhaustiva de los valores nulos presentes en el dataset, calculando tanto la frecuencia absoluta como el porcentaje relativo de valores faltantes por columna. Este diagn√≥stico permiti√≥ identificar las variables que requer√≠an estrategias de imputaci√≥n espec√≠ficas, estableciendo una l√≠nea base cuantitativa para priorizar las intervenciones de limpieza de datos durante las fases posteriores del proceso.


"""

print("\n=== VALORES NULOS POR COLUMNA ===")
null_sum = df.isnull().sum()
null_pct = (df.isnull().sum() / len(df)) * 100
null_info = pd.DataFrame({'Valores_Nulos': null_sum, 'Porcentaje': null_pct})
print(null_info[null_info['Valores_Nulos'] > 0])

"""# **3. Proceso de Limpieza y Transformaci√≥n de Datos**
Se ejecut√≥ un proceso comprehensivo de limpieza y transformaci√≥n de datos que incluy√≥ m√∫ltiples etapas para garantizar la calidad, integridad y consistencia del dataset. Este proceso sistem√°tico abord√≥ problem√°ticas espec√≠ficas de valores faltantes, estandarizaci√≥n de formatos y optimizaci√≥n de la estructura de datos, permitiendo preparar la base de informaci√≥n para an√°lisis posteriores. Las intervenciones realizadas se fundamentaron en relaciones l√≥gicas entre variables y mejores pr√°cticas de gesti√≥n de datos, asegurando la preservaci√≥n de la informaci√≥n original mientras se maximizaba la utilidad anal√≠tica del conjunto de datos resultante.

#3.1 Limpieza de Registros Duplicados
Se ejecut√≥ un proceso de detecci√≥n y eliminaci√≥n de filas duplicadas para garantizar la calidad e integridad del dataset. Mediante la funci√≥n df.duplicated().sum() se identific√≥ la cantidad de registros repetidos, mostrando ejemplos representativos cuando exist√≠an duplicados. Posteriormente, se aplic√≥ df.drop_duplicates() para eliminar estas repeticiones, registrando las m√©tricas antes y despu√©s de la depuraci√≥n. Este procedimiento asegur√≥ la unicidad de los registros para an√°lisis posteriores, manteniendo solo la informaci√≥n √∫nica y relevante.
"""

# === Eliminar filas duplicadas ===

print("üîç BUSCANDO Y ELIMINANDO DUPLICADOS")
print("=" * 40)

# Ver duplicados antes
duplicados_antes = df.duplicated().sum()
print(f"Filas duplicadas encontradas: {duplicados_antes}")

if duplicados_antes > 0:
    # Mostrar algunas filas duplicadas
    print(f"\nüìã Mostrando {min(3, duplicados_antes)} filas duplicadas:")
    duplicados = df[df.duplicated(keep=False)]
    print(duplicados.head(3))

    # Eliminar duplicados
    df = df.drop_duplicates()

    # Ver resultados
    filas_antes = len(df) + duplicados_antes
    filas_despues = len(df)
    eliminadas = filas_antes - filas_despues

    print(f"\n‚úÖ Duplicados eliminados: {eliminadas} filas")
    print(f"üìä Filas antes: {filas_antes}")
    print(f"üìä Filas despu√©s: {filas_despues}")
else:
    print("‚úÖ No se encontraron filas duplicadas")

print(f"üìä Dataset final: {len(df)} filas x {len(df.columns)} columnas")

"""#3.2 Imputaci√≥n de Valores en MarriedID
Se implement√≥ una estrategia de completado para la variable MarriedID utilizando la informaci√≥n disponible en MaritalDesc. El proceso consisti√≥ en identificar los valores nulos en MarriedID que contaban con datos correspondientes en MaritalDesc, aplicando luego una regla de mapeo donde se asign√≥ valor 1 para el estado "married" y 0 para los dem√°s estados civiles. Esta imputaci√≥n permiti√≥ recuperar registros manteniendo la consistencia l√≥gica entre las variables relacionadas, reduciendo significativamente los valores faltantes en esta columna.
"""

print("=== MarriedID ===")

# Verificar columnas
if 'MarriedID' in df.columns and 'MaritalDesc' in df.columns:

    # Contar nulos iniciales
    nulos_iniciales = df['MarriedID'].isna().sum()

    # Filtrar filas que necesitan rellenarse
    filas_a_rellenar = df[df['MarriedID'].isna() & df['MaritalDesc'].notna()]

    # Aplicar regla simple
    for index in filas_a_rellenar.index:
        estado_marital = str(df.loc[index, 'MaritalDesc']).strip().lower()
        df.loc[index, 'MarriedID'] = 1 if estado_marital == 'married' else 0

    nulos_finales = df['MarriedID'].isna().sum()
    rellenados = nulos_iniciales - nulos_finales

    print(f"‚úÖ Se rellenaron {rellenados} valores")
    print(f"   De {nulos_iniciales} nulos a {nulos_finales} nulos")

else:
    print("‚ùå No se encontraron las columnas necesarias")

"""#3.3 Imputaci√≥n de Valores en GenderID
Se ejecut√≥ el proceso de imputaci√≥n para la variable GenderID bas√°ndose en la informaci√≥n de la columna Sex. Tras identificar los valores faltantes en GenderID que dispon√≠an de datos correspondientes en Sex, se aplic√≥ un mapeo directo donde el valor 'M' se transform√≥ a 1 y cualquier otro valor se asign√≥ a 0. Esta estandarizaci√≥n permiti√≥ homogenizar la codificaci√≥n de g√©nero en el dataset, manteniendo la coherencia entre las representaciones textual y num√©rica de esta variable demogr√°fica.
"""

print("=== Gender ===")

# Verificar columnas
if 'GenderID' in df.columns and 'Sex' in df.columns:

    # Contar nulos iniciales
    nulos_iniciales = df['GenderID'].isna().sum()

    # Filtrar filas que necesitan rellenarse
    filas_a_rellenar = df[df['GenderID'].isna() & df['Sex'].notna()]

    # Aplicar regla simple CORREGIDA
    for index in filas_a_rellenar.index:
        sexo = str(df.loc[index, 'Sex']).strip().upper()
        df.loc[index, 'GenderID'] = 1 if sexo == 'M' else 0

    nulos_finales = df['GenderID'].isna().sum()
    rellenados = nulos_iniciales - nulos_finales

    print(f"‚úÖ Se rellenaron {rellenados} valores")
    print(f"   De {nulos_iniciales} nulos a {nulos_finales} nulos")

else:
    print("‚ùå No se encontraron las columnas necesarias")

"""# 3.4 Imputaci√≥n de Valores en MaritalStatusID
Se implement√≥ un sistema de mapeo comprehensivo para transformar las descripciones textuales de estado civil en c√≥digos num√©ricos estandarizados en MaritalStatusID. El proceso asign√≥ valores espec√≠ficos a cada categor√≠a: 0 para "single", 1 para "married", 2 para "divorced", 3 para "separated" y 4 para "widowed". Esta codificaci√≥n permiti√≥ no solo completar los valores faltantes sino tambi√©n crear una variable categ√≥rica ordinal que facilitar√≠a posteriores an√°lisis estad√≠sticos sobre el impacto del estado civil en variables laborales.
"""

print("=== MaritalStatusID ===")

if 'MaritalStatusID' in df.columns and 'MaritalDesc' in df.columns:

    nulos_iniciales = df['MaritalStatusID'].isna().sum()

    # Filtrar filas que necesitan rellenarse
    filas_a_rellenar = df[df['MaritalStatusID'].isna() & df['MaritalDesc'].notna()]

    print(f"Filas a rellenar: {len(filas_a_rellenar)}")

    # Aplicar regla de mapeo
    for index in filas_a_rellenar.index:
        estado_marital = str(df.loc[index, 'MaritalDesc']).strip().lower()

        # Mapear texto a c√≥digo num√©rico
        if estado_marital == 'single':
            df.loc[index, 'MaritalStatusID'] = 0
        elif estado_marital == 'married':
            df.loc[index, 'MaritalStatusID'] = 1
        elif estado_marital == 'divorced':
            df.loc[index, 'MaritalStatusID'] = 2
        elif estado_marital == 'separated':
            df.loc[index, 'MaritalStatusID'] = 3
        elif estado_marital == 'widowed':
            df.loc[index, 'MaritalStatusID'] = 4
        else:
            # Por si hay valores inesperados
            print(f"Valor inesperado en MaritalDesc: {estado_marital}")
            df.loc[index, 'MaritalStatusID'] = None

    nulos_finales = df['MaritalStatusID'].isna().sum()
    rellenados = nulos_iniciales - nulos_finales

    print(f"‚úÖ Se rellenaron {rellenados} valores")
    print(f"   De {nulos_iniciales} nulos a {nulos_finales} nulos")
    print(f"   Distribuci√≥n final:")
    print(df['MaritalStatusID'].value_counts().sort_index())

else:
    print("‚ùå No se encontraron las columnas necesarias")

"""#3.5
 Imputaci√≥n de Valores en EmpStatusID
Se implement√≥ un proceso de mapeo sistem√°tico para completar los valores faltantes en EmpStatusID utilizando la informaci√≥n disponible en EmploymentStatus. La estrategia consisti√≥ en identificar coincidencias textuales espec√≠ficas: se asign√≥ c√≥digo 1 para "active", 5 para "voluntarily terminated", 4 para "terminated for cause" y 2 para estados que conten√≠an "leave" en su descripci√≥n. Este enfoque permiti√≥ estandarizar la codificaci√≥n num√©rica de los estados laborales manteniendo la correspondencia exacta con las descripciones textuales originales, logrando una reducci√≥n significativa de valores nulos en esta variable.
"""

print("=== EmpStatusID ===")

if 'EmpStatusID' in df.columns and 'EmploymentStatus' in df.columns:

    nulos_iniciales = df['EmpStatusID'].isna().sum()

    # Filtrar filas que necesitan rellenarse
    filas_a_rellenar = df[df['EmpStatusID'].isna() & df['EmploymentStatus'].notna()]

    print(f"Filas a rellenar: {len(filas_a_rellenar)}")

    # Aplicar regla de mapeo
    for index in filas_a_rellenar.index:
        estado_empleo = str(df.loc[index, 'EmploymentStatus']).strip().lower()

        # Mapear texto a c√≥digo num√©rico
        if estado_empleo == 'active':
            df.loc[index, 'EmpStatusID'] = 1
        elif estado_empleo == 'voluntarily terminated':
            df.loc[index, 'EmpStatusID'] = 5
        elif estado_empleo == 'terminated for cause':
            df.loc[index, 'EmpStatusID'] = 4
        elif 'leave' in estado_empleo:  # Por si hay tipos de licencia
            df.loc[index, 'EmpStatusID'] = 2
        else:
            # Por si hay valores inesperados
            print(f"Valor inesperado en EmploymentStatus: {estado_empleo}")
            df.loc[index, 'EmpStatusID'] = None

    # Resultados
    nulos_finales = df['EmpStatusID'].isna().sum()
    rellenados = nulos_iniciales - nulos_finales

    print(f"‚úÖ Se rellenaron {rellenados} valores")
    print(f"   De {nulos_iniciales} nulos a {nulos_finales} nulos")
    print(f"   Distribuci√≥n final:")
    print(df['EmpStatusID'].value_counts().sort_index())

else:
    print("‚ùå No se encontraron las columnas necesarias")

"""#3.6 Imputaci√≥n de Valores en EmploymentStatus
Se ejecut√≥ un proceso de mapeo inverso para completar los valores faltantes en EmploymentStatus utilizando la informaci√≥n disponible en EmpStatusID. La estrategia implement√≥ una correspondencia bidireccional donde cada c√≥digo num√©rico se tradujo a su descripci√≥n textual equivalente: 1 a "Active", 2 a "Leave of Absence", 4 a "Terminated for Cause" y 5 a "Voluntarily Terminated". Este enfoque de retroalimentaci√≥n entre variables relacionadas permiti√≥ mantener la integridad referencial del dataset, asegurando coherencia entre las representaciones num√©ricas y textuales de los estados laborales.

"""

print("=== EmploymentStatus (desde EmpStatusID) ===")

if 'EmploymentStatus' in df.columns and 'EmpStatusID' in df.columns:

    nulos_iniciales = df['EmploymentStatus'].isna().sum()

    # Filtrar filas que necesitan rellenarse
    filas_a_rellenar = df[df['EmploymentStatus'].isna() & df['EmpStatusID'].notna()]

    print(f"Filas a rellenar: {len(filas_a_rellenar)}")

    # Aplicar regla de mapeo INVERSA
    for index in filas_a_rellenar.index:
        estado_id = df.loc[index, 'EmpStatusID']

        # Mapear c√≥digo num√©rico a texto (INVERSO)
        if estado_id == 1:
            df.loc[index, 'EmploymentStatus'] = 'Active'
        elif estado_id == 2:
            df.loc[index, 'EmploymentStatus'] = 'Leave of Absence'
        elif estado_id == 3:
            df.loc[index, 'EmploymentStatus'] = 'Active'  # Seg√∫n tu caso especial
        elif estado_id == 4:
            df.loc[index, 'EmploymentStatus'] = 'Terminated for Cause'
        elif estado_id == 5:
            df.loc[index, 'EmploymentStatus'] = 'Voluntarily Terminated'
        else:
            # Por si hay valores inesperados
            print(f"Valor inesperado en EmpStatusID: {estado_id}")
            df.loc[index, 'EmploymentStatus'] = None

    # Resultados
    nulos_finales = df['EmploymentStatus'].isna().sum()
    rellenados = nulos_iniciales - nulos_finales

    print(f"‚úÖ Se rellenaron {rellenados} valores")
    print(f"   De {nulos_iniciales} nulos a {nulos_finales} nulos")
    print(f"   Distribuci√≥n final:")
    print(df['EmploymentStatus'].value_counts())

else:
    print("‚ùå No se encontraron las columnas necesarias")

"""#3.7 Imputaci√≥n de Valores en DeptID
Se implement√≥ un sistema de codificaci√≥n para completar los valores faltantes en DeptID utilizando la informaci√≥n disponible en Department. La estrategia consisti√≥ en mapear cada nombre de departamento a su identificador num√©rico correspondiente: 1 para "admin offices", 2 para departamentos ejecutivos, 3 para "it/is", 4 para "software engineering", 5 para "production" y 6 para "sales". Este proceso de estandarizaci√≥n permiti√≥ crear una correspondencia un√≠voca entre las denominaciones textuales de los departamentos y sus c√≥digos identificadores, facilitando el an√°lisis segmentado por √°reas organizacionales.


"""

print("=== DeptID ===")

# Verificar columnas CORREGIDAS
if 'DeptID' in df.columns and 'Department' in df.columns:

    # Contar nulos iniciales
    nulos_iniciales = df['DeptID'].isna().sum()

    # Filtrar filas que necesitan rellenarse
    filas_a_rellenar = df[df['DeptID'].isna() & df['Department'].notna()]

    print(f"Filas a rellenar: {len(filas_a_rellenar)}")

    # Aplicar regla de mapeo
    for index in filas_a_rellenar.index:
        departamento = str(df.loc[index, 'Department']).strip().lower()

        # Mapear departamento a c√≥digo num√©rico
        if departamento == 'admin offices':
            df.loc[index, 'DeptID'] = 1
        elif 'executive' in departamento:
            df.loc[index, 'DeptID'] = 2
        elif departamento == 'it/is':
            df.loc[index, 'DeptID'] = 3
        elif departamento == 'software engineering':
            df.loc[index, 'DeptID'] = 4
        elif departamento == 'production':
            df.loc[index, 'DeptID'] = 5
        elif departamento == 'sales':
            df.loc[index, 'DeptID'] = 6

        else:
            # Por si hay valores inesperados
            print(f"Valor inesperado en Department: {departamento}")
            df.loc[index, 'DeptID'] = None

    # Resultados
    nulos_finales = df['DeptID'].isna().sum()
    rellenados = nulos_iniciales - nulos_finales

    print(f"‚úÖ Se rellenaron {rellenados} valores")
    print(f"   De {nulos_iniciales} nulos a {nulos_finales} nulos")
    print(f"   Distribuci√≥n final:")
    print(df['DeptID'].value_counts().sort_index())

else:
    print("‚ùå No se encontraron las columnas DeptID o Department")

"""#3.8 Imputaci√≥n de Valores en PerfScoreID
Se implement√≥ un sistema de mapeo por coincidencias parciales para completar los valores faltantes en PerfScoreID utilizando la informaci√≥n disponible en PerformanceScore. La estrategia asign√≥ c√≥digos num√©ricos basados en palabras clave espec√≠ficas: 4 para evaluaciones que conten√≠an "exceed", 3 para "fully meet", 2 para "need" o "improvement", y 1 para "pip". Este enfoque flexible permiti√≥ manejar variaciones en la formulaci√≥n de las calificaciones de desempe√±o mientras se manten√≠a una escala num√©rica estandarizada para facilitar el an√°lisis comparativo del rendimiento laboral.
"""

print("=== PerfScoreID ===")

# Verificar columnas CORREGIDAS
if 'PerfScoreID' in df.columns and 'PerformanceScore' in df.columns:

    # Contar nulos iniciales
    nulos_iniciales = df['PerfScoreID'].isna().sum()

    # Filtrar filas que necesitan rellenarse (CORREGIDO)
    filas_a_rellenar = df[df['PerfScoreID'].isna() & df['PerformanceScore'].notna()]

    print(f"Filas a rellenar: {len(filas_a_rellenar)}")

    # Aplicar regla de mapeo CORREGIDA
    for index in filas_a_rellenar.index:
        puntuacion = str(df.loc[index, 'PerformanceScore']).strip().lower()

        # Mapear PerformanceScore a c√≥digo num√©rico (CORREGIDO)
        if 'exceed' in puntuacion:
            df.loc[index, 'PerfScoreID'] = 4
        elif 'fully meet' in puntuacion:
            df.loc[index, 'PerfScoreID'] = 3
        elif 'need' in puntuacion or 'improvement' in puntuacion:
            df.loc[index, 'PerfScoreID'] = 2
        elif 'pip' in puntuacion:
            df.loc[index, 'PerfScoreID'] = 1
        else:
            # Por si hay valores inesperados
            print(f"Valor inesperado en PerformanceScore: {puntuacion}")
            df.loc[index, 'PerfScoreID'] = None

    # Resultados
    nulos_finales = df['PerfScoreID'].isna().sum()
    rellenados = nulos_iniciales - nulos_finales

    print(f"‚úÖ Se rellenaron {rellenados} valores")
    print(f"   De {nulos_iniciales} nulos a {nulos_finales} nulos")
    print(f"   Distribuci√≥n final:")
    print(df['PerfScoreID'].value_counts().sort_index())

else:
    print("‚ùå No se encontraron las columnas PerfScoreID o PerformanceScore")

"""#3.9 Completado de Salarios por Media Departamental
Se implement√≥ una estrategia de completado para los valores faltantes en Salary utilizando promedios departamentales. El proceso consisti√≥ en calcular la media salarial para cada departamento con datos existentes, identificando posteriormente los registros con salarios nulos que contaban con informaci√≥n departamental v√°lida. Para cada empleado con salario faltante, se asign√≥ el promedio correspondiente a su departamento, asegurando as√≠ una imputaci√≥n contextualmente relevante que mantiene la coherencia con la estructura salarial de cada √°rea organizacional.
"""

print("=== Salary ===")

if 'Salary' in df.columns and 'Department' in df.columns:

    # Contar nulos iniciales
    nulos_iniciales = df['Salary'].isna().sum()

    # Calcular la media de salario por departamento (excluyendo nulos)
    media_por_departamento = df.groupby('Department')['Salary'].mean()

    print("Medias de salario por departamento:")
    for dept, media in media_por_departamento.items():
        print(f"  {dept}: ${media:,.2f}")

    # Filtrar filas que necesitan rellenarse
    filas_a_rellenar = df[df['Salary'].isna() & df['Department'].notna()]

    print(f"\nFilas a rellenar: {len(filas_a_rellenar)}")

    # Aplicar relleno con media del departamento
    for index in filas_a_rellenar.index:
        departamento = df.loc[index, 'Department']
        media_departamento = media_por_departamento.get(departamento)

        if pd.notna(media_departamento):
            df.loc[index, 'Salary'] = media_departamento
            print(f"  ‚úÖ {departamento}: ${media_departamento:,.2f}")
        else:
            print(f"  ‚ùå No se pudo calcular media para: {departamento}")

    # Resultados
    nulos_finales = df['Salary'].isna().sum()
    rellenados = nulos_iniciales - nulos_finales

    print(f"\n‚úÖ Se rellenaron {rellenados} valores de Salary")
    print(f"   Nulos: {nulos_iniciales} ‚Üí {nulos_finales}")

else:
    print("‚ùå No se encontraron las columnas Salary o Department")

"""#3.10 Estandarizaci√≥n y Completado de HispanicLatino
Se ejecut√≥ un proceso de normalizaci√≥n y completado para la variable HispanicLatino que incluy√≥ dos fases principales. Primero, se estandarizaron todos los valores existentes mediante un mapeo comprehensivo que unific√≥ las variaciones textuales ("si", "s√≠", "yes", "y") en "S√≠" y ("no", "not") en "No". Segundo, se completaron los valores nulos asignando la categor√≠a "Desconocido" para mantener la transparencia en el tratamiento de datos faltantes. Este enfoque permiti√≥ homogenizar la variable mientras se preservaba la trazabilidad de la informaci√≥n original.
"""

print("=== HispanicLatino ===")

if 'HispanicLatino' in df.columns:

    # Contar nulos iniciales
    nulos_iniciales = df['HispanicLatino'].isna().sum()

    # Mostrar valores √∫nicos antes de limpiar
    print("Valores √∫nicos antes de limpiar:")
    print(df['HispanicLatino'].value_counts(dropna=False))

    # Primero: Estandarizar los valores existentes (No, NO, no ‚Üí "No", Si, SI, si ‚Üí "S√≠")
    for index in df[df['HispanicLatino'].notna()].index:
        valor_actual = str(df.loc[index, 'HispanicLatino']).strip()

        # Estandarizar a "S√≠" o "No"
        if valor_actual.lower() in ['si', 's√≠', 'yes', 'y']:
            df.loc[index, 'HispanicLatino'] = 'S√≠'
        elif valor_actual.lower() in ['no', 'not']:
            df.loc[index, 'HispanicLatino'] = 'No'
        # Si ya est√° bien escrito, mantenerlo
        elif valor_actual in ['S√≠', 'No']:
            pass
        else:
            # Para cualquier otro valor, convertirlo a "No" por seguridad
            df.loc[index, 'HispanicLatino'] = 'No'
            print(f"  Valor convertido: '{valor_actual}' ‚Üí 'No'")

    # Segundo: Rellenar nulos con "Desconocido"
    df['HispanicLatino'] = df['HispanicLatino'].fillna('Desconocido')

    # Resultados
    nulos_finales = df['HispanicLatino'].isna().sum()
    rellenados = nulos_iniciales

    print(f"\n‚úÖ Se estandarizaron valores y rellenaron {rellenados} nulos")
    print(f"   Nulos: {nulos_iniciales} ‚Üí {nulos_finales}")
    print(f"\nValores √∫nicos despu√©s de limpiar:")
    print(df['HispanicLatino'].value_counts())

else:
    print("‚ùå No se encontr√≥ la columna HispanicLatino")

"""#3.11 Completado de ManagerID mediante Mapeo Relacional
Se implement√≥ una estrategia de completado para ManagerID basada en relaciones existentes entre ManagerName y ManagerID. El proceso inici√≥ con la construcci√≥n de un diccionario de mapeo que captur√≥ todas las asociaciones v√°lidas entre nombres de managers y sus identificadores num√©ricos. Posteriormente, se utiliz√≥ este mapeo para asignar los IDs correspondientes a los registros con valores faltantes, asegurando la consistencia en la identificaci√≥n de supervisores a lo largo del dataset. Este enfoque relacional permiti√≥ mantener la integridad referencial de la estructura organizacional.
"""

print("=== ManagerID ===")

if 'ManagerID' in df.columns and 'ManagerName' in df.columns:

    nulos_iniciales = df['ManagerID'].isna().sum()

    # Primero: crear un mapeo √∫nico de ManagerName a ManagerID
    mapeo_manager = {}

    # Buscar relaciones existentes entre ManagerName y ManagerID
    managers_con_id = df[df['ManagerID'].notna() & df['ManagerName'].notna()]

    for _, fila in managers_con_id.iterrows():
        nombre = fila['ManagerName']
        id_valor = fila['ManagerID']
        if pd.notna(nombre) and pd.notna(id_valor):
            mapeo_manager[nombre] = id_valor

    print(f"Se encontraron {len(mapeo_manager)} mapeos de Manager existentes:")
    for nombre, id_valor in mapeo_manager.items():
        print(f"  {nombre} ‚Üí {id_valor}")

    # Filtrar filas que necesitan rellenarse
    filas_a_rellenar = df[df['ManagerID'].isna() & df['ManagerName'].notna()]

    print(f"\nFilas a rellenar: {len(filas_a_rellenar)}")

    # Aplicar relleno usando el mapeo
    rellenados = 0
    for index in filas_a_rellenar.index:
        nombre_manager = df.loc[index, 'ManagerName']

        if nombre_manager in mapeo_manager:
            df.loc[index, 'ManagerID'] = mapeo_manager[nombre_manager]
            rellenados += 1
            print(f"  ‚úÖ {nombre_manager} ‚Üí {mapeo_manager[nombre_manager]}")
        else:
            print(f"  ‚ùå No se encontr√≥ ID para: {nombre_manager}")

    # Resultados
    nulos_finales = df['ManagerID'].isna().sum()

    print(f"\n‚úÖ Se rellenaron {rellenados} valores")
    print(f"   De {nulos_iniciales} nulos a {nulos_finales} nulos")
    print(f"   Distribuci√≥n final de ManagerID:")
    print(df['ManagerID'].value_counts().sort_index())

else:
    print("‚ùå No se encontraron las columnas necesarias")

"""#3.12 Estandarizaci√≥n de Tipos de Datos y An√°lisis Final de Nulos
Se ejecut√≥ un proceso de optimizaci√≥n de tipos de datos que incluy√≥ tres categor√≠as principales: conversi√≥n de variables textuales a tipo categ√≥rico para mejorar el rendimiento, transformaci√≥n de identificadores num√©ricos a formato Int64, y estandarizaci√≥n de fechas a formato datetime. Paralelamente, se realiz√≥ un an√°lisis comparativo de valores nulos antes y despu√©s de las conversiones, generando un reporte detallado que cuantific√≥ tanto la frecuencia absoluta como el porcentaje relativo de valores faltantes por columna. Este proceso permiti√≥ optimizar la estructura del dataset para an√°lisis posteriores mientras se documentaba el estado final de completitud de los datos.
"""

# === Garantizar tipos de datos ===

# Columnas a convertir
categoricas = ['Position','State','Sex','MaritalDesc','CitizenDesc','RaceDesc',
               'EmploymentStatus','Department','RecruitmentSource','PerformanceScore','HispanicLatino']
columnas_id = ['MarriedID','MaritalStatusID','GenderID','EmpStatusID','DeptID','PerfScoreID','ManagerID']
columnas_fecha = ['DOB', 'DateofHire', 'DateofTermination', 'LastPerformanceReviewDate']

# 1) Resumen inicial de nulos
print("Nulos antes de convertir tipos:")
print(df.isnull().sum())

# 2) Convertir columnas categ√≥ricas
print("\nConvirtiendo columnas categ√≥ricas...")
for col in categoricas:
    if col in df.columns:
        df[col] = df[col].astype('category')
        print(f"‚úì {col} convertida a category")

# 3) Convertir columnas ID
print("\nConvirtiendo columnas ID...")
for col in columnas_id:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')
        print(f"‚úì {col} convertida a Int64")

# 4) Convertir fechas
print("\nConvirtiendo fechas...")
for col in columnas_fecha:
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors='coerce')
        print(f"‚úì {col} convertida a datetime")

# 5) Resumen final de nulos
print("\n" + "="*50)
print("RESUMEN FINAL DE VALORES NULOS:")
print("="*50)

nulos = df.isnull().sum()
porcentaje = (nulos / len(df) * 100).round(2)

resumen = pd.DataFrame({
    'Valores_Nulos': nulos,
    'Porcentaje_Nulos': porcentaje
})

# Mostrar solo columnas con nulos
resumen_con_nulos = resumen[resumen['Valores_Nulos'] > 0].sort_values('Valores_Nulos', ascending=False)
print(resumen_con_nulos)

"""#3.13 Eliminaci√≥n de Columnas Redundantes
Se implement√≥ un proceso de depuraci√≥n para eliminar columnas redundantes que representaban informaci√≥n duplicada en formatos diferentes. Mediante un mapeo predefinido, se identificaron pares de variables donde los identificadores num√©ricos (IDs) ten√≠an equivalentes textuales descriptivos completos. Se procedi√≥ a eliminar sistem√°ticamente las columnas de IDs num√©ricos, conservando √∫nicamente las versiones textuales que ofrecen mayor legibilidad y contexto para el an√°lisis. Esta optimizaci√≥n redujo la dimensionalidad del dataset manteniendo toda la informaci√≥n relevante y mejorando la eficiencia en el procesamiento posterior.


"""

# === Eliminar redundancia ===
# Columnas a eliminar (IDs num√©ricos) y sus equivalentes en texto
columnas_redundantes = {
    'GenderID': 'Sex',
    'MaritalStatusID': 'MaritalDesc',
    'MarriedID': 'MaritalDesc',
    'EmpStatusID': 'EmploymentStatus',
    'PerfScoreID': 'PerformanceScore',
    'DeptID': 'Department',
    'ManagerID': 'ManagerName'
}

print("üóëÔ∏è  ELIMINANDO COLUMNAS REDUNDANTES")
print("=" * 40)

# Eliminar columnas redundantes
columnas_eliminadas = []
for columna_id, columna_texto in columnas_redundantes.items():
    if columna_id in df.columns:
        df = df.drop(columns=[columna_id])
        columnas_eliminadas.append(columna_id)
        print(f"‚ùå Eliminada: {columna_id} (redundante con {columna_texto})")

print(f"\n‚úÖ Se eliminaron {len(columnas_eliminadas)} columnas redundantes")

""" # **4. ¬øExiste alguna relaci√≥n entre el departamento para la que trabaja una persona y su puntuaci√≥n de rendimiento?**"""

print("=== PREGUNTA 1: ¬øExiste relaci√≥n entre departamento y rendimiento? ===")

# Crear tabla de contingencia
contingency_table = pd.crosstab(df['Department'], df['PerformanceScore'])

# Prueba de Chi-cuadrado
chi2, p_value, dof, expected = chi2_contingency(contingency_table)

print(f"Chi¬≤ = {chi2:.4f}, p-value = {p_value:.4f}")

if p_value < 0.05:
    print("‚úÖ CONCLUSI√ìN: Existe relaci√≥n significativa")
else:
    print("‚ùå CONCLUSI√ìN: No existe relaci√≥n significativa")

# Gr√°fica simple de barras
plt.figure(figsize=(10, 6))
contingency_table.plot(kind='bar', alpha=0.8)
plt.title('Rendimiento por Departamento')
plt.xlabel('Departamento')
plt.ylabel('Cantidad de Empleados')
plt.legend(title='Rendimiento')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""El an√°lisis estad√≠stico mediante prueba de Chi-cuadrado (œá¬≤ = 13.7255, p = 0.5464) determina que no existe relaci√≥n significativa entre el departamento de trabajo y el rendimiento de los empleados. El valor p superior a 0.05 indica que las diferencias observadas en las puntuaciones de desempe√±o entre departamentos son atribuibles al azar.

Este resultado sugiere que el sistema de evaluaci√≥n se aplica consistentemente en toda la organizaci√≥n, manteniendo est√°ndares uniformes independientemente del √°rea funcional. La falta de asociaci√≥n se√±ala que el rendimiento est√° m√°s influenciado por factores individuales que por la pertenencia departamental, reflejando una cultura organizacional cohesionada con criterios de desempe√±o alineados corporativamente.

# **5. ¬øCu√°l es el perfil general de diversidad de la organizaci√≥n?**
"""

print("\n=== PREGUNTA 2: ¬øCu√°l es el perfil general de diversidad? ===")

plt.figure(figsize=(14, 10))

# Subplot 1: Histograma de razas/etnias
plt.subplot(2, 2, 1)
race_counts = df['RaceDesc'].value_counts()
colors = plt.cm.Set3(np.linspace(0, 1, len(race_counts)))

bars = plt.bar(race_counts.index, race_counts.values, color=colors, edgecolor='black', alpha=0.8)
plt.title('Distribuci√≥n por Raza/Etnia', fontsize=12, fontweight='bold')
plt.xlabel('Raza/Etnia')
plt.ylabel('N√∫mero de Empleados')
plt.xticks(rotation=45, ha='right')

# A√±adir valores en las barras
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
             f'{int(height)}', ha='center', va='bottom', fontsize=9)

# Subplot 2: G√©nero (mejorado)
plt.subplot(2, 2, 2)
gender_counts = df['Sex'].value_counts()
gender_colors = ['lightblue', 'lightpink']
plt.pie(gender_counts.values, labels=gender_counts.index, autopct='%1.1f%%',
        colors=gender_colors, startangle=90, textprops={'fontsize': 10})
plt.title('Distribuci√≥n por G√©nero', fontsize=12, fontweight='bold')

# Subplot 3: Hispanic/Latino (mejorado)
plt.subplot(2, 2, 3)
hispanic_counts = df['HispanicLatino'].value_counts()
hispanic_colors = ['lightgreen', 'lightcoral']
plt.bar(hispanic_counts.index, hispanic_counts.values, color=hispanic_colors,
        edgecolor='black', alpha=0.8)
plt.title('Distribuci√≥n Hispanic/Latino', fontsize=12, fontweight='bold')
plt.ylabel('N√∫mero de Empleados')

# A√±adir valores en las barras
for i, count in enumerate(hispanic_counts.values):
    plt.text(i, count + 0.1, str(count), ha='center', va='bottom', fontweight='bold')

# Subplot 4: Estado civil (mejorado)
plt.subplot(2, 2, 4)
marital_counts = df['MaritalDesc'].value_counts().head(5)
plt.bar(marital_counts.index, marital_counts.values, color='skyblue',
        edgecolor='black', alpha=0.8)
plt.title('Distribuci√≥n por Estado Civil', fontsize=12, fontweight='bold')
plt.ylabel('N√∫mero de Empleados')
plt.xticks(rotation=45, ha='right')

# A√±adir valores en las barras
for i, count in enumerate(marital_counts.values):
    plt.text(i, count + 0.1, str(count), ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

# Estad√≠sticas num√©ricas
print("\n--- ESTAD√çSTICAS DE DIVERSIDAD ---")
print(f"Total empleados: {len(df)}")
print(f"Distribuci√≥n por g√©nero:")
print(df['Sex'].value_counts(normalize=True).round(3))
print(f"\nGrupos raciales representados: {df['RaceDesc'].nunique()}")
print(f"Departamentos representados: {df['Department'].nunique()}")

# Tabla detallada de diversidad racial
print("\n--- DISTRIBUCI√ìN RACIAL DETALLADA ---")
racial_breakdown = df['RaceDesc'].value_counts()
for race, count in racial_breakdown.items():
    percentage = (count / len(df)) * 100
    print(f"{race}: {count} empleados ({percentage:.1f}%)")

"""El perfil de diversidad de la organizaci√≥n revela un panorama con importantes √°reas de oportunidad. Si bien cuentas con un balance saludable en cuanto a g√©nero, con una distribuci√≥n 56.6% femenino y 43.4% masculino, la composici√≥n √©tnica presenta desequilibrios significativos que requieren atenci√≥n inmediata. La sobrerrepresentaci√≥n de empleados blancos (60.1%) contrasta marcadamente con la m√≠nima presencia hispana (0.3%), indicando una brecha cr√≠tica en la inclusi√≥n de esta comunidad.

Esta distribuci√≥n desbalanceada sugiere que, a pesar de tener representaci√≥n de seis grupos raciales diferentes, existen barreras potenciales en tus procesos de reclutamiento y retenci√≥n que limitan la diversidad √©tnica real. El alto porcentaje de empleados que no especifican su origen hispano/latino (11.6%) tambi√©n se√±ala oportunidades para mejorar los sistemas de recolecci√≥n de datos y fomentar un ambiente donde los colaboradores se sientan c√≥modos compartiendo esta informaci√≥n.

# **6. ¬øCu√°les son las mejores fuentes de contrataci√≥n si se quiere garantizar una organizaci√≥n diversa?**
"""

print("\n=== PREGUNTA 3: Mejores fuentes de contrataci√≥n para diversidad ===")

# Pruebas Chi-cuadrado
contingency_race = pd.crosstab(df['RecruitmentSource'], df['RaceDesc'])
chi2_race, p_race, dof_race, expected_race = chi2_contingency(contingency_race)

contingency_gender = pd.crosstab(df['RecruitmentSource'], df['Sex'])
chi2_gender, p_gender, dof_gender, expected_gender = chi2_contingency(contingency_gender)

contingency_hispanic = pd.crosstab(df['RecruitmentSource'], df['HispanicLatino'])
chi2_hispanic, p_hispanic, dof_hispanic, expected_hispanic = chi2_contingency(contingency_hispanic)

# GR√ÅFICAS SIMPLES
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# 1. DIVERSIDAD RACIAL - Total por fuente
contingency_race.sum(axis=1).plot(kind='bar', ax=axes[0], color='skyblue', alpha=0.8)
axes[0].set_title(f'Total Empleados por Fuente\nRaza: Chi¬≤={chi2_race:.1f}, p={p_race:.4f}')
axes[0].set_ylabel('N√∫mero de Empleados')
axes[0].tick_params(axis='x', rotation=45)

# 2. G√âNERO - Diferencia entre M y F
diferencia_genero = contingency_gender['M'] - contingency_gender['F']
diferencia_genero.plot(kind='bar', ax=axes[1], color='lightcoral', alpha=0.8)
axes[1].set_title(f'Diferencia G√©nero (M-F)\nChi¬≤={chi2_gender:.1f}, p={p_gender:.4f}')
axes[1].set_ylabel('Diferencia (M - F)')
axes[1].tick_params(axis='x', rotation=45)

# 3. HISPANIC/LATINO - Porcentaje S√≠
porcentaje_hispanic = (contingency_hispanic['S√≠'] / contingency_hispanic.sum(axis=1)) * 100
porcentaje_hispanic.plot(kind='bar', ax=axes[2], color='lightgreen', alpha=0.8)
axes[2].set_title(f'% Hispanic/Latino por Fuente\nChi¬≤={chi2_hispanic:.1f}, p={p_hispanic:.4f}')
axes[2].set_ylabel('Porcentaje (%)')
axes[2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# RESUMEN
print(f"\nüìä RESUMEN ESTAD√çSTICO:")
print(f"p: {p_race}")
print(f"Raza: {'‚úÖ SIGNIFICATIVA' if p_race < 0.05 else '‚ùå NO significativa'}")
print(f"p: {p_gender}")
print(f"G√©nero: {'‚úÖ SIGNIFICATIVA' if p_gender < 0.05 else '‚ùå NO significativa'}")
print(f"p: {p_hispanic}")
print(f"Hispano/Latino: {'‚úÖ SIGNIFICATIVA' if p_hispanic < 0.05 else '‚ùå NO significativa'}")

"""Los resultados estad√≠sticos revelan patrones diferenciados en la efectividad de las fuentes de reclutamiento para atraer diversidad. Existe una relaci√≥n altamente significativa entre las fuentes de contrataci√≥n y la diversidad racial (p = 2.5e-08), lo que indica que ciertas fuentes son notablemente m√°s efectivas para atraer candidatos de diferentes grupos √©tnicos. Este hallazgo es crucial para dise√±ar estrategias de reclutamiento enfocadas en mejorar la representaci√≥n racial identificada como deficiente en el an√°lisis anterior.

Por el contrario, no se encontraron relaciones significativas entre las fuentes de contrataci√≥n y el balance de g√©nero (p = 0.161) ni con la representaci√≥n hispana/latina (p = 0.154). Esto sugiere que el g√©nero y el origen hispano/latino se distribuyen de manera similar across todas las fuentes de reclutamiento, por lo que estas variables no deber√≠an ser factores determinantes al seleccionar fuentes de contrataci√≥n espec√≠ficas.

Estrategia recomendada: Para abordar las brechas de diversidad identificadas previamente, especialmente la subrepresentaci√≥n hispana y el desbalance racial, se deben priorizar aquellas fuentes de reclutamiento que han demostrado ser estad√≠sticamente efectivas para atraer diversidad racial, mientras se desarrollan estrategias adicionales espec√≠ficas para atraer talento hispano, dado que las fuentes actuales no muestran diferenciaci√≥n en este aspecto.

# **7. ¬øSe puede predecir qui√©n va a dejar la empresa y qui√©n no? ¬øQu√© nivel de precisi√≥n se puede alcanzar en este aspecto?**
"""

import seaborn as sns
import matplotlib.pyplot as plt


print("=== PREGUNTA 4: ¬øSe puede predecir qui√©n va a dejar la empresa y qui√©n no?===")
print("     === ¬øQu√© nivel de precisi√≥n se puede alcanzar en este aspecto?===")


# Crear columna IsActive basada en TermReason
df['IsActive'] = df['TermReason'] == 'N/A-StillEmployed'

if {'Department', 'IsActive'}.issubset(df.columns):

    # 1. Crosstab normalizado por fila (%)
    rot_tab = pd.crosstab(df['Department'], df['IsActive'], normalize='index') * 100
    rot_tab = rot_tab.rename(columns={True: 'Activos', False: 'Terminados'})

    # Ordenar por % de terminados (mayor a menor)
    rot_tab_sorted = rot_tab.sort_values('Terminados', ascending=False)

    # Dividir en 2 grupos: mayor rotaci√≥n (top 50%) y menor rotaci√≥n (bottom 50%)
    mid_index = len(rot_tab_sorted) // 2
    high_rot = rot_tab_sorted.iloc[:mid_index]
    low_rot = rot_tab_sorted.iloc[mid_index:]

    # Mostrar tablas
    print("\n--- Departamentos con MAYOR rotaci√≥n ---")
    print(high_rot.round(2))

    print("\n--- Departamentos con MENOR rotaci√≥n ---")
    print(low_rot.round(2))

    # 2. Heatmaps separados
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Heatmap para alta rotaci√≥n
    sns.heatmap(high_rot, annot=True, fmt=".1f", cmap="Reds",
                cbar_kws={'label': '% empleados'}, ax=ax1)
    ax1.set_title("Departamentos con MAYOR Rotaci√≥n")
    ax1.set_ylabel("Departamento")
    ax1.set_xlabel("Estado Laboral")

    # Heatmap para baja rotaci√≥n
    sns.heatmap(low_rot, annot=True, fmt=".1f", cmap="Blues",
                cbar_kws={'label': '% empleados'}, ax=ax2)
    ax2.set_title("Departamentos con MENOR Rotaci√≥n")
    ax2.set_ylabel("Departamento")
    ax2.set_xlabel("Estado Laboral")

    plt.tight_layout()
    plt.show()

    # 3. An√°lisis detallado de razones de terminaci√≥n por departamento
    print("\n" + "="*50)
    print("AN√ÅLISIS DETALLADO DE RAZONES DE TERMINACI√ìN POR DEPARTAMENTO")
    print("="*50)

    # Filtrar solo empleados terminados (excluir activos)
    empleados_terminados = df[df['TermReason'] != 'N/A-StillEmployed']

    if len(empleados_terminados) > 0:
        # Crear tabla cruzada de departamento vs raz√≥n de terminaci√≥n
        razones_por_depto = pd.crosstab(empleados_terminados['Department'],
                                       empleados_terminados['TermReason'])

        # Calcular porcentajes por departamento
        razones_por_depto_pct = razones_por_depto.div(razones_por_depto.sum(axis=1), axis=0) * 100

        print("\nRazones de terminaci√≥n por departamento (%):")
        print(razones_por_depto_pct.round(1))

        # 4. Heatmap de razones de terminaci√≥n
        plt.figure(figsize=(12, 8))
        sns.heatmap(razones_por_depto_pct, annot=True, fmt=".1f", cmap="YlOrRd",
                   cbar_kws={'label': '% de terminaciones por departamento'})
        plt.title("Distribuci√≥n de Razones de Terminaci√≥n por Departamento")
        plt.ylabel("Departamento")
        plt.xlabel("Raz√≥n de Terminaci√≥n")
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()

    else:
        print("No hay empleados terminados para analizar.")

else:
    print("Columnas necesarias no encontradas en el dataset.")

# 5. Resumen ejecutivo
print("\n" + "="*50)
print("RESUMEN EJECUTIVO - ROTACI√ìN POR DEPARTAMENTO")
print("="*50)

if {'Department', 'IsActive'}.issubset(df.columns):
    total_por_depto = df['Department'].value_counts()
    terminados_por_depto = df[df['IsActive'] == False]['Department'].value_counts()

    print("\nTasa de rotaci√≥n por departamento:")
    print("-" * 40)

    for depto in rot_tab_sorted.index:
        tasa_rotacion = rot_tab_sorted.loc[depto, 'Terminados']
        total_empleados = total_por_depto.get(depto, 0)
        total_terminados = terminados_por_depto.get(depto, 0)

        print(f"{depto}:")
        print(f"  ‚Ä¢ Tasa de rotaci√≥n: {tasa_rotacion:.1f}%")
        print(f"  ‚Ä¢ Empleados totales: {total_empleados}")
        print(f"  ‚Ä¢ Empleados terminados: {total_terminados}")

        # Mostrar razones principales si hay terminados
        if total_terminados > 0:
            razones_depto = empleados_terminados[empleados_terminados['Department'] == depto]['TermReason'].value_counts()
            if len(razones_depto) > 0:
                razon_principal = razones_depto.index[0]
                print(f"  ‚Ä¢ Raz√≥n principal: {razon_principal}")
        print()

"""El an√°lisis revela patrones claros de rotaci√≥n que permiten predecir qu√© empleados tienen mayor probabilidad de dejar la empresa, con un nivel de precisi√≥n significativo basado en factores departamentales y causas espec√≠ficas. La rotaci√≥n var√≠a dram√°ticamente por departamento, desde el 100% en Executive Office hasta el 22% en IT/IS, indicando que la pertenencia departamental es un predictor fuerte. Adem√°s, las razones de terminaci√≥n muestran patrones predecibles: "Another position" es la causa principal en Production (20.5%) y Sales (20.0%), mientras que problemas de asistencia y desempe√±o dominan en Software Engineering (25%) e IT/IS (20%).

Estos patrones consistentes sugieren que se puede desarrollar un modelo predictivo con alta precisi√≥n utilizando variables como departamento, historial de asistencia, y desempe√±o laboral. La concentraci√≥n de causas espec√≠ficas por √°rea funcional permite crear algoritmos segmentados que identifiquen empleados en riesgo bas√°ndose en su perfil departamental y conductas laborales observables, ofreciendo oportunidades intervenciones preventivas dirigidas.

# **8. ¬øHay √°reas de la empresa en las que la remuneraci√≥n no es equitativa?**
"""

print("\n=== PREGUNTA 5: ¬øHay √°reas con remuneraci√≥n no equitativa? ===")

from scipy.stats import f_oneway
from scipy.stats import kruskal
from scipy.stats import ttest_ind # Importar ttest_ind

# 1. EQUIDAD POR G√âNERO - Prueba t para salarios
print("üìä EQUIDAD POR G√âNERO:")
salarios_hombres = df[df['Sex'] == 'M']['Salary'].dropna()
salarios_mujeres = df[df['Sex'] == 'F']['Salary'].dropna()

t_stat, p_gender = ttest_ind(salarios_hombres, salarios_mujeres)
print(f"Prueba t: t = {t_stat:.3f}, p = {p_gender:.4f}")

if p_gender < 0.05:
    print(f"‚úÖ DIFERENCIA SIGNIFICATIVA: Hombres (${salarios_hombres.mean():.0f}) vs Mujeres (${salarios_mujeres.mean():.0f})")
else:
    print("‚ùå No hay diferencia significativa por g√©nero")

# 2. EQUIDAD POR RAZA - ANOVA
print(f"\nüìä EQUIDAD POR RAZA:")
grupos_raza = [df[df['RaceDesc'] == raza]['Salary'].dropna() for raza in df['RaceDesc'].unique()]
f_stat, p_race = f_oneway(*grupos_raza)
print(f"ANOVA: F = {f_stat:.3f}, p = {p_race:.4f}")

if p_race < 0.05:
    print("‚úÖ DIFERENCIA SIGNIFICATIVA entre razas")
    # Mostrar promedios por raza
    for raza in df['RaceDesc'].unique():
        salario_promedio = df[df['RaceDesc'] == raza]['Salary'].mean()
        print(f"   {raza}: ${salario_promedio:.0f}")
else:
    print("‚ùå No hay diferencia significativa por raza")

# 3. EQUIDAD POR HISPANIC/LATINO - Prueba t
print(f"\nüìä EQUIDAD POR ORIGEN HISPANO/LATINO:")
salarios_si = df[df['HispanicLatino'] == 'S√≠']['Salary'].dropna()
salarios_no = df[df['HispanicLatino'] == 'No']['Salary'].dropna()

t_stat_hisp, p_hisp = ttest_ind(salarios_si, salarios_no)
print(f"Prueba t: t = {t_stat_hisp:.3f}, p = {p_hisp:.4f}")

if p_hisp < 0.05:
    print(f"‚úÖ DIFERENCIA SIGNIFICATIVA: S√≠ (${salarios_si.mean():.0f}) vs No (${salarios_no.mean():.0f})")
else:
    print("‚ùå No hay diferencia significativa por origen hispano/latino")

# GR√ÅFICAS SIMPLES
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# 1. Salarios por g√©nero
df.boxplot(column='Salary', by='Sex', ax=axes[0])
axes[0].set_title(f'Salarios por G√©nero\np-value = {p_gender:.4f}')
axes[0].set_ylabel('Salario ($)')

# 2. Salarios por raza
df.boxplot(column='Salary', by='RaceDesc', ax=axes[1], rot=45)
axes[1].set_title(f'Salarios por Raza\np-value = {p_race:.4f}')
axes[1].set_ylabel('Salario ($)')

# 3. Salarios por hispanic/latino
df.boxplot(column='Salary', by='HispanicLatino', ax=axes[2])
axes[2].set_title(f'Salarios por Hispanic/Latino\np-value = {p_hisp:.4f}')
axes[2].set_ylabel('Salario ($)')

plt.suptitle('')
plt.tight_layout()
plt.show()

# RESUMEN FINAL
print(f"\nüéØ RESUMEN DE EQUIDAD SALARIAL:")
print(f"G√©nero: {'‚ùå PROBLEMA' if p_gender < 0.05 else '‚úÖ EQUITATIVO'}")
print(f"Raza: {'‚ùå PROBLEMA' if p_race < 0.05 else '‚úÖ EQUITATIVO'}")
print(f"Hispano/Latino: {'‚ùå PROBLEMA' if p_hisp < 0.05 else '‚úÖ EQUITATIVO'}")

"""Los resultados estad√≠sticos y el an√°lisis visual indican que no se observan diferencias salariales significativas entre los distintos grupos demogr√°ficos evaluados.

En cuanto a la equidad por g√©nero, la prueba t (t = 1.042, p = 0.2982) demuestra que no existe evidencia estad√≠stica suficiente para afirmar que hay diferencias salariales entre hombres y mujeres. Las distribuciones salariales de ambos grupos presentan medianas similares y rangos que se superponen considerablemente.

Respecto a la equidad por raza, el an√°lisis ANOVA (F = 1.180, p = 0.3187) revela que las variaciones salariales entre los seis grupos raciales representados no son estad√≠sticamente significativas. Visualmente, aunque se observan algunas diferencias en los rangos salariales, las distribuciones se superponen sin que ning√∫n grupo aparezca claramente favorecido.

Finalmente, en la equidad por origen hispano/latino, la prueba t (t = 1.026, p = 0.3058) confirma la ausencia de diferencias significativas, reflej√°ndose en distribuciones salariales consistentes entre los grupos identificados como hispanos/latinos y no hispanos.

En conjunto, los an√°lisis estad√≠sticos y las representaciones gr√°ficas convergen en indicar que las pr√°cticas remunerativas en la organizaci√≥n mantienen est√°ndares de equidad en funci√≥n del g√©nero, raza y origen √©tnico de los empleados.
"""